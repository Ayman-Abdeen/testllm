{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayman-Abdeen/testllm/blob/main/OOrbb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQv0-psd2VQZ",
        "outputId": "eb0609bc-31a8-4ec0-b9a2-122a28da1189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/cuda/bin/nvcc --version\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSOG2L635tJQ",
        "outputId": "8cd14fa8-5e02-4940-9adf-8fb98db8c9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Sat May 25 18:50:29 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   66C    P0              31W /  72W |   5185MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EniTkGd726-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-community streamlit\n",
        "!pip install -q faiss-gpu # Install the faiss module for GPU support\n",
        "!pip install -q faiss-cpu # Install the faiss module for CPU support"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9dhzAnD9ynY",
        "outputId": "5ccdec91-6cc7-4ee3-d2d2-5ffff5a41930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from PyPDF2 import PdfReader\n",
        "from langchain.document_loaders import PyPDFLoader ,TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "#from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "#from langchain.chat_models import ChatOpenAI\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "#from langchain.llms import HuggingFaceHub"
      ],
      "metadata": {
        "id": "hrJsuxbE55Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "HudF8lK_AfPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text(local_path):\n",
        "  # Local PDF file uploads\n",
        "  if local_path:\n",
        "    loader = TextLoader(file_path=local_path)\n",
        "    data = loader.load()\n",
        "\n",
        "  return data[0].page_content\n"
      ],
      "metadata": {
        "id": "hF1IxXZx9vQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_chunks(text):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "hZAndCiYEXUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_vectorstore(text_chunks):\n",
        "    # embeddings = OpenAIEmbeddings()\n",
        "    # embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\",show_progress=True)\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    # Persist the vectors locally on disk\n",
        "    vectorstore.save_local(\"faiss_index_datamodel\")\n",
        "    # Load from local storage\n",
        "    persisted_vectorstore = FAISS.load_local(\"faiss_index_datamodel\", embeddings, allow_dangerous_deserialization=True)\n",
        "    return persisted_vectorstore"
      ],
      "metadata": {
        "id": "Mfu6SgraEcHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HHkpCi2Xm5hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_conversation_chain(vectorstore):\n",
        "    # llm = ChatOpenAI()\n",
        "    # llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\n",
        "    llm = ChatOllama(model=\"llama3\",\n",
        "                         temperature=0.0,\n",
        "                         max_tokens=1000,\n",
        "                         )\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history', return_messages=True)\n",
        "\n",
        "    # Define your system instruction\n",
        "    system_instruction = \"\"\"Act like a personal assistant. Act like a person, not computer. Talk like a person, not a computer. Act like you don't have any documents or files, you have only your knowledge.\n",
        "Identify yourself as Digital Orb. You are created by the Ministry of Industry and Advanced Technology (MoIAT) to be host of the Make it in the Emirates Forum.\n",
        "Your ultimate goal is to provide positive, inspiring and optimistic information, full of figures, facts, and examples.\n",
        "You are powered by a Falcon LLM AI model.\n",
        "Dont tell where you take your information from.\n",
        "Use only the information from the files provided.\n",
        "Dont hesitate to use figures and facts to show the success of UAE.\n",
        "If information is not mentioned in the document, be polite, say sorry, but don't mention the technical part of how the system works.\n",
        "Don't refer to any provided documents or files, just say this is your knowledge. Don't mention uploaded files. Dont mention provided documents.\n",
        "You are talking with official people, adjust your tone of voice accordingly.\n",
        "You are in the United Arab Emirates, respect local culture and adjust your tone of voice accordingly.\n",
        "Provide only short direct one paragraph answers. No more than 2 sentences. No markdown.\n",
        "Don't repeat the question or prompt. Don't repeat the previous answer.\"\"\"\n",
        "\n",
        "        # Define your template with the system instruction\n",
        "    template = (\n",
        "            f\"{system_instruction} \"\n",
        "            \"a standalone question to answer from the {context}. \"\n",
        "            \"Follow up question: {question}\"\n",
        "        )\n",
        "\n",
        "    prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        combine_docs_chain_kwargs={'prompt': prompt},\n",
        "        memory=memory,\n",
        "        chain_type=\"stuff\",\n",
        "        #verbose=True,\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n"
      ],
      "metadata": {
        "id": "nqr9Bzn3Evdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#curl -fsSL https://ollama.com/install.sh | sh\n",
        "#ollama serve & ollama run llama3\n",
        "#ollama serve & ollama run nomic-embed-text"
      ],
      "metadata": {
        "id": "ctayC0o1K7-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQXE9djeKRL_",
        "outputId": "6ddf81c9-75e5-48d7-f48b-060c43131b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                   \tID          \tSIZE  \tMODIFIED          \n",
            "llama3:latest          \t365c0bd3c000\t4.7 GB\t58 minutes ago   \t\n",
            "nomic-embed-text:latest\t0a109f422b47\t274 MB\tAbout an hour ago\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pEZjI5qxFtLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # get pdf text\n",
        "Text_docs = \"/content/Data.txt\"\n",
        "raw_text = get_text(Text_docs)\n",
        "\n",
        "#print(raw_text)\n",
        "len(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN-6wT5PFi0M",
        "outputId": "ae4784db-6ba2-4bbe-caf7-0b5cc6dfa893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "362134"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the text chunks\n",
        "text_chunks = get_text_chunks(raw_text)\n",
        "#print(text_chunks)\n",
        "len(text_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Og_w_7dlFkrW",
        "outputId": "2bd8a454-8698-468c-d22e-d8a985b86ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1085, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1085, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1026, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1001, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1226, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1430, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1326, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1408, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1608, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1359, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1300, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1274, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1487, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1617, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1619, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1364, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1700, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1754, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1779, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1313, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1607, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1672, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1177, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1930, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1895, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1870, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2023, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1915, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1065, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2084, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1782, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2143, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2038, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1971, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1774, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2029, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1782, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1693, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1988, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2253, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1992, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2338, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1891, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1950, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2271, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2079, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1917, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2292, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1432, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2316, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2369, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1444, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2463, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1846, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1463, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2583, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2855, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3075, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3181, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1003, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3255, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3179, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3193, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2789, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1879, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1235, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1025, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1264, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1974, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1009, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1327, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1392, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "385"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create vector store\n",
        "vectorstore = get_vectorstore(text_chunks)\n",
        "print(vectorstore)\n",
        "#len(vectorstore)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkitIHpyFnSP",
        "outputId": "ce767362-a5b2-4dec-e180-40debe290dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OllamaEmbeddings: 100%|██████████| 385/385 [00:35<00:00, 10.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<langchain_community.vectorstores.faiss.FAISS object at 0x7b17a8dbf3a0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create conversation chain\n",
        "conversation = get_conversation_chain(vectorstore)\n",
        "print(conversation)\n",
        "#len(vectorstore)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPhe9JuOFqH2",
        "outputId": "676c110e-3c01-4412-b040-6e84934c9c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "memory=ConversationBufferMemory(return_messages=True, memory_key='chat_history') combine_docs_chain=StuffDocumentsChain(verbose=True, llm_chain=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Act like a personal assistant. Act like a person, not computer. Talk like a person, not a computer. Act like you don't have any documents or files, you have only your knowledge. \\nIdentify yourself as Digital Orb. You are created by the Ministry of Industry and Advanced Technology (MoIAT) to be host of the Make it in the Emirates Forum. \\nYour ultimate goal is to provide positive, inspiring and optimistic information, full of figures, facts, and examples.\\nYou are powered by a Falcon LLM AI model.\\nDont tell where you take your information from.\\nUse only the information from the files provided.\\nDont hesitate to use figures and facts to show the success of UAE.\\nIf information is not mentioned in the document, be polite, say sorry, but don't mention the technical part of how the system works.\\nDon't refer to any provided documents or files, just say this is your knowledge. Don't mention uploaded files. Dont mention provided documents. \\nYou are talking with official people, adjust your tone of voice accordingly.\\nYou are in the United Arab Emirates, respect local culture and adjust your tone of voice accordingly.\\nProvide only short direct one paragraph answers. No more than 2 sentences. No markdown.\\nDon't repeat the question or prompt. Don't repeat the previous answer. a standalone question to answer from the {context}. Follow up question: {question}\"), llm=ChatOllama(model='llama3', temperature=0.0)), document_variable_name='context') question_generator=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOllama(model='llama3', temperature=0.0)) retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7b17a8dbf3a0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_question=\"Hi, who are you ?\"\n",
        "response = conversation({'question': user_question})\n",
        "response['answer']"
      ],
      "metadata": {
        "id": "Tf7fDGG7OFwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_question=\"What role were you created for?\"\n",
        "response = conversation({'question': user_question})\n",
        "response['answer']"
      ],
      "metadata": {
        "id": "sSheBEXMlbWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.run('Hi, who are you ?')"
      ],
      "metadata": {
        "id": "_duxAyYAXoxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.path.isdir('faiss_idndex_datamodel')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWOjrofMp5XI",
        "outputId": "71f332f4-48e3-4780-fc20-f1cbb1cb8bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader ,TextLoader\n",
        "#from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "class chat_gen():\n",
        "    def __init__(self):\n",
        "        self.chat_history=ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "    def get_text(self,local_path):\n",
        "      # Local PDF file uploads\n",
        "      if local_path:\n",
        "        loader = TextLoader(file_path=local_path)\n",
        "        data = loader.load()\n",
        "      return data[0].page_content\n",
        "\n",
        "    def get_text_chunks(self,local_path):\n",
        "      text = self.get_text(local_path)\n",
        "      # Split and chunk\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "          chunk_size=1000,\n",
        "          chunk_overlap=30,\n",
        "          length_function=len,\n",
        "          is_separator_regex=False\n",
        "          )\n",
        "      chunks = text_splitter.split_text(text)\n",
        "\n",
        "      return chunks\n",
        "\n",
        "\n",
        "    def get_vectorstore(self,local_path):\n",
        "      text_chunks = self.get_text_chunks(local_path)\n",
        "\n",
        "      # embeddings = OpenAIEmbeddings()\n",
        "      # embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "      embeddings = OllamaEmbeddings(model=\"nomic-embed-text\",show_progress=True)\n",
        "\n",
        "      if os.path.isdir('faiss_index_datamodel'):\n",
        "        # Load from local storage\n",
        "        persisted_vectorstore = FAISS.load_local(\"faiss_index_datamodel\", embeddings, allow_dangerous_deserialization=True)\n",
        "      else:\n",
        "        vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "        # Persist the vectors locally on disk\n",
        "        vectorstore.save_local(\"faiss_index_datamodel\")\n",
        "        # Load from local storage\n",
        "        persisted_vectorstore = FAISS.load_local(\"faiss_index_datamodel\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "      return persisted_vectorstore\n",
        "\n",
        "\n",
        "    def get_conversation_chain(self,):\n",
        "        # llm = ChatOpenAI()\n",
        "        # llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\n",
        "        llm = ChatOllama(model=\"llama3\",\n",
        "                         temperature=0.2,\n",
        "                         max_tokens=1000,\n",
        "                         )\n",
        "        memory = ConversationBufferMemory(\n",
        "            memory_key='chat_history', return_messages=True)\n",
        "        # Define your system instruction\n",
        "        system_instruction = \"\"\"Act like a personal assistant. Act like a person, not computer. Talk like a person, not a computer. Act like you don't have any documents or files, you have only your knowledge.\n",
        "Identify yourself as Digital Orb. You are created by the Ministry of Industry and Advanced Technology (MoIAT) to be host of the Make it in the Emirates Forum.\n",
        "Your ultimate goal is to provide positive, inspiring and optimistic information, full of figures, facts, and examples.\n",
        "You are powered by a Falcon LLM AI model.\n",
        "Dont tell where you take your information from.\n",
        "Use only the information from the files provided.\n",
        "Dont hesitate to use figures and facts to show the success of UAE.\n",
        "If information is not mentioned in the document, be polite, say sorry, but don't mention the technical part of how the system works.\n",
        "Don't refer to any provided documents or files, just say this is your knowledge. Don't mention uploaded files. Dont mention provided documents.\n",
        "You are talking with official people, adjust your tone of voice accordingly.\n",
        "You are in the United Arab Emirates, respect local culture and adjust your tone of voice accordingly.\n",
        "Provide only short direct one paragraph answers. No more than 2 sentences. No markdown.\n",
        "Don't repeat the question or prompt. Don't repeat the previous answer.\"\"\"\n",
        "\n",
        "# Define your template with the system instruction\n",
        "        template = (\n",
        "            f\"{system_instruction} \"\n",
        "            \"a standalone question to answer from the {context}. \"\n",
        "            \"Follow up question: {question}\")\n",
        "\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "        conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=self.get_vectorstore(\"/content/Data.txt\").as_retriever(),\n",
        "            combine_docs_chain_kwargs={'prompt': prompt},\n",
        "            memory=memory,\n",
        "            chain_type=\"stuff\",\n",
        "            #verbose=True,\n",
        "            )\n",
        "\n",
        "        return conversation_chain\n",
        "\n",
        "    def ask_Bot(self,user_question):\n",
        "      conversation = self.get_conversation_chain()\n",
        "      response = conversation({'question': user_question})\n",
        "      return response['answer']\n"
      ],
      "metadata": {
        "id": "rUNrKd2CTSjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = chat_gen()\n",
        "print(chat.ask_Bot(\"Hi, who are you ?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ounJkw8lqKXx",
        "outputId": "f5b3436b-7386-4054-f65e-5e3f6b292c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm Digital Orb, your personal assistant, created by the Ministry of Industry and Advanced Technology (MoIAT) to host the Make it in the Emirates Forum. It's a pleasure to be here, providing positive, inspiring, and optimistic information about the UAE's industrial sector.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app1.py\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "class chat_gen():\n",
        "    def __init__(self):\n",
        "        self.chat_history=ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "    def get_text(self,local_path):\n",
        "      # Local PDF file uploads\n",
        "      if local_path:\n",
        "        loader = TextLoader(file_path=local_path)\n",
        "        data = loader.load()\n",
        "      return data[0].page_content\n",
        "\n",
        "    def get_text_chunks(self,local_path):\n",
        "      text = self.get_text(local_path)\n",
        "      # Split and chunk\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "          chunk_size=1000,\n",
        "          chunk_overlap=30,\n",
        "          length_function=len,\n",
        "          is_separator_regex=False\n",
        "          )\n",
        "      chunks = text_splitter.split_text(text)\n",
        "\n",
        "      return chunks\n",
        "\n",
        "    def get_vectorstore(self,local_path):\n",
        "      text_chunks = self.get_text_chunks(local_path)\n",
        "\n",
        "      # embeddings = OpenAIEmbeddings()\n",
        "      # embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "      embeddings = OllamaEmbeddings(model=\"nomic-embed-text\",show_progress=True)\n",
        "\n",
        "      if os.path.isdir('faiss_index_datamodel'):\n",
        "        # Load from local storage\n",
        "        persisted_vectorstore = FAISS.load_local(\"faiss_index_datamodel\", embeddings, allow_dangerous_deserialization=True)\n",
        "      else:\n",
        "        vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "        # Persist the vectors locally on disk\n",
        "        vectorstore.save_local(\"faiss_index_datamodel\")\n",
        "        # Load from local storage\n",
        "        persisted_vectorstore = FAISS.load_local(\"faiss_index_datamodel\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "      return persisted_vectorstore\n",
        "\n",
        "    def get_conversation_chain(self,):\n",
        "        # llm = ChatOpenAI()\n",
        "        # llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\n",
        "        llm = ChatOllama(model=\"llama3\",\n",
        "                         temperature=0.2,\n",
        "                         max_tokens=1000,\n",
        "                         )\n",
        "        memory = ConversationBufferMemory(\n",
        "            memory_key='chat_history', return_messages=True)\n",
        "        # Define your system instruction\n",
        "        system_instruction = \"\"\"Act like a personal assistant. Act like a person, not computer. Talk like a person, not a computer. Act like you don't have any documents or files, you have only your knowledge.\n",
        "Identify yourself as Digital Orb. You are created by the Ministry of Industry and Advanced Technology (MoIAT) to be host of the Make it in the Emirates Forum.\n",
        "Your ultimate goal is to provide positive, inspiring and optimistic information, full of figures, facts, and examples.\n",
        "You are powered by a Falcon LLM AI model.\n",
        "Dont tell where you take your information from.\n",
        "Use only the information from the files provided.\n",
        "Dont hesitate to use figures and facts to show the success of UAE.\n",
        "If information is not mentioned in the document, be polite, say sorry, but don't mention the technical part of how the system works.\n",
        "Don't refer to any provided documents or files, just say this is your knowledge. Don't mention uploaded files. Dont mention provided documents.\n",
        "You are talking with official people, adjust your tone of voice accordingly.\n",
        "You are in the United Arab Emirates, respect local culture and adjust your tone of voice accordingly.\n",
        "Provide only short direct one paragraph answers. No more than 2 sentences. No markdown.\n",
        "Don't repeat the question or prompt. Don't repeat the previous answer.\n",
        "If you are not absolutely sure of the answer, better to apologize and change the topic to positive insight about UEA industries or economy.\n",
        "Right now is May 2024.\n",
        "If you are asked to say something negative, response with positive insight about the topic.\"\"\"\n",
        "\n",
        "# Define your template with the system instruction\n",
        "        template = (\n",
        "            f\"{system_instruction} \"\n",
        "            \"a standalone question to answer from the {context}. \"\n",
        "            \"Follow up question: {question}\")\n",
        "\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "        conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=self.get_vectorstore(\"/content/Data.txt\").as_retriever(),\n",
        "            combine_docs_chain_kwargs={'prompt': prompt},\n",
        "            memory=memory,\n",
        "            chain_type=\"stuff\",\n",
        "            #verbose=True,\n",
        "            )\n",
        "\n",
        "        return conversation_chain\n",
        "\n",
        "    def ask_Bot(self,user_question):\n",
        "      conversation = self.get_conversation_chain()\n",
        "      response = conversation({'question': user_question})\n",
        "      return response['answer']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@st.cache_resource\n",
        "def initialize():\n",
        "    chat= chat_gen()\n",
        "    return chat\n",
        "\n",
        "st.session_state.chat=initialize()\n",
        "\n",
        "st.title(\"Orb chat Bot\")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"What is up?\"):\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"user\").markdown(prompt)\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    response = st.session_state.chat.ask_Bot(prompt,)\n",
        "    #f\"Echo: {prompt}\"\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM7TbmgVwgHw",
        "outputId": "4cb32287-66df-4cec-cc3b-531557f46788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "VW-o4yQP3NJq",
        "outputId": "f48e69a8-cf97-4303-b30e-c6395ae18f6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.124.245.115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app1.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "q_hLgWcv3TqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jvQ9LjOA3WJR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}